\section{Introduction}

The quest for better Genetic Programming (GP) based algorithms---by overcoming known drawbacks, proposing new paradigms or hybridizing existing ones---is as important as the search for better ways of evaluating these methods and comparing them under different aspects. It is necessary to consistently and efficiently identify the scenarios where the new algorithm excels and how it compares to its predecessors.

The definition of a concise testbed for GP is a recurring matter in GP community. In the 14th Genetic and Evolutionary Computation Conference, \citet{mcdermott2012genetic} brought to light the need for a redefinition of the datasets employed by GP community. This work resulted in an extended study \cite{white2013better}, which listed possible good and bad choices for benchmarking in GP.

\citet{nicolau2015guidelines} analysed symbolic regression datasets presented by \citet{mcdermott2012genetic}, comparing different aspects of these benchmarks. They focused on data synthetically generated datasets---the data is generated by applying a given function to a set of predefined input points---exploring the sampling strategy and size and the addition of artificial noise to the data. 

The authors compared GP, Grammatical Evolution to two particularly simple baselines---a constant, corresponding to the average response observed in the training set and a linear regression model. 





The objective of this works was role introduce an insight for an analytical model to evaluate benchmarks used by GP comunity, in order to improve the way GP based methods are evaluated in the literature.



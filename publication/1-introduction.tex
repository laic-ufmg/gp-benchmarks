\section{Introduction}

The quest for better Genetic Programming (GP) based algorithms---e.g., capable of overcoming known drawbacks, presenting new interesting properties or operating with less resources---is as important as the search for better ways of evaluating these methods and comparing them under different aspects. It is necessary to consistently and efficiently identify the scenarios where the new algorithm excels and how it compares to its predecessors. Otherwise, we may be led to the wrong conclusions, using the wrong method to solve the problem at hand.

The definition of a concise testbed for GP is a recurring matter in the research community. In the 14th Genetic and Evolutionary Computation Conference, \citet{mcdermott2012genetic} brought to light the need for a redefinition of the datasets employed by GP community. This work resulted in an extended study \cite{white2013better}, which listed possible good and bad choices for benchmarking in GP. However, the quality of the benchmarks was measured subjectively using the feedback obtained from a GP community survey and the 14th GECCO.

This work takes a different direction by analysing the datasets employed by GP community using meta-features captured from the data. The main objective is to present a direction for a framework to measure the quality of the benchmarks quantitatively. Although previous works have already analysed GP datasets under a quantitative perspective \cite{nicolau2015guidelines,dick2015reexamination}, to the best of our knowledge this is the first work to use meta-features.








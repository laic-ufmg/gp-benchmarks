{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recovering the data\n",
    "Notice that the datasets are loaded directly from the web. The links for the data are available in our paper and were found from other papers using the dataset. We provide no warranties or conditions of any kind to the availability of these datasets.\n",
    "\n",
    "## 2.1 Dealing with HTTP Error 403\n",
    "Some website tries to prevent content scraping. In order to overcome the scraping protection, we used a different http request headers, defined inside the function read_csv_from_web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_web(url, sep = ','):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36' \n",
    "                         '(KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36', \n",
    "                'Referer': 'https://www.nseindia.com', \n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'}\n",
    "    url_request  = Request(url, headers = headers)\n",
    "    return pd.read_csv(urlopen(url_request), sep = sep)\n",
    "\n",
    "\n",
    "\n",
    "tower_df = read_csv_from_web(\"\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 URL for the data\n",
    "These links were recovered from the papers and were active on the date we executed this scrip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4999, 26)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_urls = {'towerData' : \"http://symbolicregression.com/sites/default/files/DataSets/towerData.txt\",\n",
    "             'toxicity' : \"http://personal.disco.unimib.it/Vanneschi/toxicity.txt\",\n",
    "             'CC' : \"http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data\"\n",
    "             'DLBCL' : \"https://llmpp.nih.gov/DLBCL/DLBCL_patient_data_NEW.txt\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Communities and Crime (CCun and CCn)\n",
    "These datasets are used in the paper \"*Feature Selection to Improve Generalization of Genetic Programming for High-Dimensional Symbolic Regression*\". The authors applied some pre-processing to the data before using in their experiemnts (this information was obtained directly from the authors, since it is not present in the paper):\n",
    "1. Discard the first 5 features, since they are non-predictive features.\n",
    "2. From the 18 potential goal/target, we only interested in “ViolentCrimesPerPop” and discard the other 17 features.\n",
    "3. Use a simple imputation method (set the missing value to be the mean value of the feature) for instances where the feature values are missing,\n",
    "4. Discard the instances where the target value is missing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
